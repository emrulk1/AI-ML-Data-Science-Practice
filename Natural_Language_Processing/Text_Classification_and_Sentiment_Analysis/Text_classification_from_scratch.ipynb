{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Suppress specific warnings (e.g., DeprecationWarnings)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data: IMDB movie review sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0   792k      0  0:01:43  0:01:43 --:--:--  945k77k      0  0:02:01  0:00:23  0:01:38  771k0   741k      0  0:01:50  0:00:33  0:01:17 1074k 798k      0  0:01:42  0:01:35  0:00:07  469k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the compressed aclImdb_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar xvzf aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sneak-Peak on dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README\timdb.vocab  imdbEr.txt\ttest  train\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34maclImdb\u001b[0m\n",
      "├── README\n",
      "├── imdb.vocab\n",
      "├── imdbEr.txt\n",
      "├── \u001b[01;34mtest\u001b[0m\n",
      "│   ├── labeledBow.feat\n",
      "│   ├── \u001b[01;34mneg\u001b[0m\n",
      "│   ├── \u001b[01;34mpos\u001b[0m\n",
      "│   ├── urls_neg.txt\n",
      "│   └── urls_pos.txt\n",
      "└── \u001b[01;34mtrain\u001b[0m\n",
      "    ├── labeledBow.feat\n",
      "    ├── \u001b[01;34mneg\u001b[0m\n",
      "    ├── \u001b[01;34mpos\u001b[0m\n",
      "    ├── \u001b[01;34munsup\u001b[0m\n",
      "    ├── unsupBow.feat\n",
      "    ├── urls_neg.txt\n",
      "    ├── urls_pos.txt\n",
      "    └── urls_unsup.txt\n",
      "\n",
      "7 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "# !tree aclImdb\n",
    "!tree -L 2 aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
      "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view a sinlge text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/6248_7.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the pos and neg subfolders, so let's delete the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 625\n",
      "Number of batches in raw_val_ds: 157\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "# raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "# # raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "# # raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"What is interesting is that the acting; was not bad, just not enough. It was rather lame., special effects nor the lines were the single culprit for this failure. Standing alone they weren't horribly bad, but put together was a tragic move. The show seemed long winded and slow with special effects apparently designed to speed the movie along, but it failed totally.<br /><br />Much of the blame for this disaster was put on special effects.Don't believe it, they were kinda cool. Appleby was not the best choice for this endeavor. Though she may have been all they had to chose from with a bit of fan recogniton. An experienced actress would have brought something to the part, like Appleby never did. Scfi puts out some really good original movies, it's just too bad that this failed so drastically.\"\n",
      "0\n",
      "b\"I wasn't particularly impressed by this movie that has lackluster music and only lasts 40 minutes. Thank God, because I was falling asleep. I makes excellent use of time lapse photography to display the passage of time in the movement of light and shadow, people, water, clouds, etc. Unfortunately, that's all it is.<br /><br />My preference is for its predecessor, the excellent Koyaanisqatsi made in 1983 at 87 minutes and to prove that a sequel can be better than the original, Powaqqatsi made in 1988 running 90 minutes.<br /><br />Try them both.\"\n",
      "0\n",
      "b\"As incredible as it may seem, Gojoe is an anime- and Hong Kong-inspired samurai action flick with a pacifistic message. This ankle of the film is effectively portrayed through the protagonist (a great acting job done by Daisuke Ryu), a killer-turned-to-boddhist-monk Benkei. Benkei has sworn never to kill again, but he still takes up the sword to fight what he thinks is a demon invasion...<br /><br />Gojoe is a film difficult to rate. It's visual imagery is stunningly crafted and beautiful, but it uses too much trickery (circling camera and high speed drives, expressionistic shots, leeched colors, digital effects etc.), so the end result is somewhat tiring. That said, the beginning and the ending of the film are nevertheless both elegant and powerful. If only the director Sogo Ishii would have been wise enough not to overuse his bag of tricks.<br /><br />Other problem with Gojoe is the amount of violence. For a film with such an anti-violent message Gojoe wastes way too much energy and screen time to depict the endless battle scenes. Also, the way the violence is shown is always on the edge of being self-indulgent; in fact, a blood shower against the night sky seems to be one of the films signature images. Luckily, Ishii is wise enough to show the ugly, tragic side of violence as well. Still, it seems that Ishii is not sure whether he's making a traditional action film or a deeply moral allegory. The audience can't be sure of this, either, until the very end of the film. The powerful (albeit cynical) ending is what saves Gojoe; it clearly emphasizes that this film is something more than a mere gore-fest.\"\n",
      "1\n",
      "b'This movie was well done in all respects. The acting is superb along with the fine audio soundtrack which I purchased because it was so moving. It is my all time favorite movie ahead of eastwoods \"white hunter,black heart\". This movie is simply the best.<br /><br />cheers Zuf'\n",
      "1\n",
      "b'I feel it is my duty as a lover of horror films to warm other people about this horrible and very very bad \"horror\" film. Don\\'t waste your time or money on this film, the acting is bad, the story is just one of the worst i have come across and the script was just awful. Nothing about it was good, you end up thinking to yourself why am i watching this crap. The plot had so many holes in it and they never got cleared up in the end, it was just so bad, i don\\'t know how a film so terrible could be made. As i said before i love horror films and i was so let down, it was an 18 but you see little blood and no scares or jumps at all. Also what annoyed me was how stupid things happened in the film that had no point to the plot at all like the brother and sister kissing, why? is all i can say. Just don\\'t bother, there are far more great horror films out there, just don\\'t waste your time life is too short.'\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 11:49:32.904877: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "# This is one of the places where eager execution shines:\n",
    "# we can just evaluate these tensors using .numpy()\n",
    "# instead of needing to evaluate them in a Session/Graph context.\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#In particular, we remove <br /> tags.\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n",
    "# for tensorflow 2.4 or lower\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two options to vectorize the data\n",
    "\n",
    "There are 2 ways we can use our text vectorization layer:\n",
    "\n",
    "### Option 1: \n",
    "Make it part of the model, so as to obtain a model that processes raw strings, like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "x = vectorize_layer(text_input)\n",
    "x = tf.keras.layers.Embedding(max_features + 1, embedding_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: \n",
    "#### Apply it to the text dataset \n",
    "to obtain a dataset of word indices, then feed it into a model that expects integer sequences as inputs.\n",
    "\n",
    "An important difference between the two is that option 2 enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So if you're training the model on GPU, you probably want to go with this option to get the best performance. This is what we will do below.\n",
    "\n",
    "If we were to export our model to production, we'd ship a model that accepts raw strings as input, like in the code snippet for option 1 above. This can be done after training. We do this in the last section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model\n",
    "\n",
    "We choose a simple 1D convnet starting with an Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - accuracy: 0.5907 - loss: 0.6220 - val_accuracy: 0.8646 - val_loss: 0.3213\n",
      "Epoch 2/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.8895 - loss: 0.2779 - val_accuracy: 0.8772 - val_loss: 0.3044\n",
      "Epoch 3/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9481 - loss: 0.1432 - val_accuracy: 0.8688 - val_loss: 0.3413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f30ac194d60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8645 - loss: 0.3577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36220863461494446, 0.8621600270271301]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an end-to-end model\n",
    "If you want to obtain a model capable of processing raw strings, you can simply create a new model (using the weights we just trained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.8651 - loss: 0.3556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36220860481262207, 0.8621600270271301]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- https://keras.io/examples/nlp/text_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra codes for testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a string which we want to vectorize\"\n",
    "#max_features = 1 --> ValueError: If set, `max_tokens` must be greater than 1.\n",
    "\n",
    "max_features = 2\n",
    "embedding_dim = 128\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorize_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(625, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds.cardinality())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_batch, label_batch \u001b[38;5;241m=\u001b[39m raw_train_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = raw_train_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
