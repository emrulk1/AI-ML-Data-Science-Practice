{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_uaE3vxqW6p"
      },
      "source": [
        "### Text Preprocessing Steps\n",
        "- **Segmentation**: Also known as sentence tokenization, is a process of breaking down a large block of text into individual sentences. It makes it easier to analyze and extract useful information from the text.\n",
        "- **Removal of punctuations, special characters and URLs**: These are noise in text and may confuse machine learning models. Should be removed to improve the quality of the text.\n",
        "- **Lowercasing**: Otherwise machine will consider upercase and lowercase same word as different words.\n",
        "- **Tokenization**: word tokenization, separates each word of a sentence, even punctuation is treated as a separate word. Useful for details analysis of each word in the text.\n",
        "- **Parts of Speech Tagging**: POS tagging involves classifying each word of a sentence into its respective parts of speech. Useful for understanding the cotext and relationships between words more accurately. Thus helps to understand the semantic meaning of the text.\n",
        "- **Removing Stopwords**: Often do not carry significant meaning and are typically discarded. EX: a, an, the, etc. These might also create noise in the train data if not removed.\n",
        "- **Text Normalization**\n",
        "- **Stemming**\n",
        "- **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7V9d4ydrJnw",
        "outputId": "beaad869-af91-41df-f471-8e8977b2c0d1"
      },
      "outputs": [],
      "source": [
        "!pip install -q nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhVJT4p1uwqP",
        "outputId": "623f6728-85e2-4d24-c6a3-b2ef10c7db87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/emrulk1/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbeM9H0RuXbm",
        "outputId": "3ef53deb-9a4f-48d2-8e00-9d1cce74161d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: The weather is nice today.\n",
            "Sentence 2: I think I'll go for a walk.\n",
            "Sentence 3: Maybe I'll take the dog with me.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"The weather is nice today. I think I'll go for a walk. Maybe I'll take the dog with me.\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "# Perform Sentence segmentation\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Print the tokenized(segmented) sentences\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUq-TSE60nnS",
        "outputId": "5e4d9975-99dd-4819-f961-2139d3e21097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned Text: Hello Check out my website  Its awesome excited user 100\n",
            "Tokenized Text: ['Hello', 'Check', 'out', 'my', 'website', 'Its', 'awesome', 'excited', 'user', '100']\n",
            "\n",
            "Cleaned and Lowercased Text: hello check out my website  its awesome excited user 100\n",
            "Tokenized Text: ['hello', 'check', 'out', 'my', 'website', 'its', 'awesome', 'excited', 'user', '100']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Make sure to download the necessary resources\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello! Check out my website: http://example.com. It's awesome! excited @user $100.\"\n",
        "\n",
        "# Define a regular expression pattern to remove URLS\n",
        "url_pattern = r\"http\\S+|www\\S+\"\n",
        "\n",
        "# Define a regular expression pattern to remove punctuations and special characters\n",
        "# punctuation_pattern = r\"[^\\w\\s]\"\n",
        "punctuation_pattern = r\"[^a-zA-Z0-9\\s]\"\n",
        "\n",
        "# Remove URLs\n",
        "cleaned_text = re.sub(url_pattern, '', text)\n",
        "\n",
        "# Remove punctuations and special characters\n",
        "cleaned_text = re.sub(punctuation_pattern, '', cleaned_text)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "# Print the cleaned text and tokenized text\n",
        "print(\"Cleaned Text:\", cleaned_text)\n",
        "print(\"Tokenized Text:\", tokens)\n",
        "print()\n",
        "\n",
        "# Print the cleaned, lowercased and tokenized text\n",
        "cleaned_text = cleaned_text.lower()\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "print(\"Cleaned and Lowercased Text:\", cleaned_text)\n",
        "print(\"Tokenized Text:\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCWJ8u1KXNqS",
        "outputId": "21ac5980-37fe-464d-e9fc-93d73ecdce42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['Text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ],
      "source": [
        "# Sample sentence\n",
        "sentence = \"Text preprocessing is an important step in natural language processing.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dmct1zlZNqW",
        "outputId": "42aa1aa9-63ce-413b-ceb9-72dc372f1f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pos Tagged Tokens: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "#sample text\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "#tokenize the sentence into words\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "#perform part-of-speech tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "#print the pos tagged tokens\n",
        "print(\"Pos Tagged Tokens:\", tagged_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwH8GWoTtKg8"
      },
      "source": [
        "## References\n",
        "- https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
        "- https://www.geeksforgeeks.org/python-perform-sentence-segmentation-using-spacy/\n",
        "- https://www.kaggle.com/discussions/general/331411 \n",
        "- https://stackoverflow.com/questions/35861482/nltk-lookup-error\n",
        "- https://github.com/cocktailpeanutlabs/openvoice2/issues/3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_dl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
