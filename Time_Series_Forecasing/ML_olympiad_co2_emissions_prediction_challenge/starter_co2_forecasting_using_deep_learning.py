# -*- coding: utf-8 -*-
"""Copy of Starter Co2 Forecasting using deep learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMeg-d1GLkyMDT5iV6nYHpWiRm_KXK3n
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'ml-olympiad-co2-emissions-prediction-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F70983%2F7794824%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240309%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240309T155145Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2a94de3497409d910b437f26e0d8372d3cbde2f5d8733bbbfe171452e15b0041fc13357b5b60624818ae0423ea2e4ef19e19d82f25cb86575cf56503053b74768187ec41d4917b92ea7356347e29d55ee571bd828896dd1b4f042a7bfc696b86a2b9117f109dccafc6021ec6e791accd1a747478bf1284eb5fd157539a33e268373317c747e28370f2a0a35957495cc13a30ace2152966551626bd62681af4c76a279c439afb2f3cfcef92022a87852a7e6bb21e30542cb5703702c5fa4ad7908f51ce3af4177f22812fcb719996de084ca538a2371a04c33dd0744d9945c7c0c33c4aaec5ce0dece77c9eb3405692f9ee61ab0fa0d5c5b5a00a3ac0432254f9'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
Train=pd.read_csv('/kaggle/input/ml-olympiad-co2-emissions-prediction-challenge/train.csv')
Test= pd.read_csv('/kaggle/input/ml-olympiad-co2-emissions-prediction-challenge/test.csv')
sample_submission= pd.read_csv('/kaggle/input/ml-olympiad-co2-emissions-prediction-challenge/sample_submission.csv')

#Filter BY CO2 emissions (metric tons per capita)
specific_string = 'CO2 emissions (metric tons per capita)'
filtered_rows = Train[Train['Indicator'] == specific_string]
filtered_rows

#Drop Indicator Column
filtered_rows.drop(columns=['Indicator'], inplace=True)
filtered_rows

#Drop Country Code Column
filtered_rows.drop(columns=['Country Code'], inplace=True)
filtered_rows

#Covert Year Column in rows
melted_df = filtered_rows.melt(id_vars='Country Name', var_name='Year', value_name='CO2_Emissions')
melted_df

#Sort Row in accending order by Year and Country Name
sorted_df = melted_df.groupby('Country Name').apply(lambda x: x.sort_values('Year')).reset_index(drop=True)
sorted_df.head(20)

#All country name into unique_values
unique_values = melted_df['Country Name'].unique()
unique_values.shape

# Replace ".." with a new value
new_value = 0.00
sorted_df = sorted_df.replace('..', new_value)

sorted_df.head(15)

#Remove unnecessary character.
import re
def remove_brackets(text):
    return re.sub(r"\[.*?\]", "", text)

for index, row in sorted_df.iterrows():
    sorted_df.at[index, 'Year'] = remove_brackets(row['Year'])
print(sorted_df)

#Converting a Year to Datetime Format
sorted_df['Year'] = sorted_df['Year'].str.strip()
sorted_df['Year'] = pd.to_datetime(sorted_df['Year'], format='%Y')

afg=sorted_df.head(16)

afg

afg=afg[['CO2_Emissions']]

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load your yearly time series data
# Assuming your data is stored in a pandas DataFrame with a single column named 'value'

for j in unique_values:
    afg = sorted_df[sorted_df['Country Name'] == j]


    print(j)
    country_data=afg['CO2_Emissions'].astype(float)
    country_data_array = country_data.values
    country_data_array = country_data_array.reshape(-1, 1)
    # Normalize the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(country_data_array)

    # Prepare the data for LSTM
    def create_dataset(data, time_steps):
        X, y = [], []
        for i in range(len(data) - time_steps):
            X.append(data[i:(i + time_steps), 0])
            y.append(data[i + time_steps, 0])
        return np.array(X), np.array(y)

    # Choose the number of time steps
    time_steps = 15 # Using data from the past 16 years to predict the next 10 years

    # Create the dataset
    X, y = create_dataset(scaled_data, time_steps)

    # Reshape input data for LSTM
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # Define the LSTM model
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))
    model.add(LSTM(units=50))
    model.add(Dense(units=1))

    # Compile the model
    model.compile(optimizer='adam',loss='mean_squared_error')

    # Train the model
    # model.fit(X, y, epochs=50, batch_size=32)
    # model.fit(X, y, epochs=50, batch_size=128)
    model.fit(X, y, epochs=100, batch_size=32, verbose=2)

    # Forecasting for the next 10 years
    forecast = []

     # Use the last `time_steps` data points from the original data to start forecasting
    current_batch = scaled_data[-time_steps:].reshape((1, time_steps, 1))

    for i in range(16):
       # Predict the next year
       next_year = model.predict(current_batch)[0, 0]
       # Append the predicted value to the forecast
       forecast.append(next_year)
       # Update the current batch with the predicted value
       current_batch = np.append(current_batch[:, 1:, :], np.array([[[next_year]]]), axis=1)
       # Reshape the predicted value to match the shape of the current batch
       next_year = np.array(next_year).reshape(1, 1, 1)

       # Inverse transform the forecast to get actual values
    forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))
    row_index = sample_submission[sample_submission.eq(j).any(axis=1)].index[0]


    new_values = [j, forecast[0][0], forecast[1][0],forecast[2][0], forecast[3][0], forecast[4][0],forecast[15][0]]
    sample_submission.loc[row_index] = new_values

sample_submission

sample_submission.to_csv('submission.csv', index=False)